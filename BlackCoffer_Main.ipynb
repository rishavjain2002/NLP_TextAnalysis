{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "KJexxRoqgXYi",
    "outputId": "7b9c5fea-da9e-4a6f-9574-6e69c3f61813"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('input.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2vA4gD-nWRw"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def url_to_transcript(url):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    # Find the article title\n",
    "    title_tag = soup.find('h1')\n",
    "    if title_tag:\n",
    "        title = title_tag.text.strip()\n",
    "    else:\n",
    "        title = None\n",
    "\n",
    "    parent_element = soup.find('div', class_='td-post-content')\n",
    "\n",
    "    if parent_element:\n",
    "        paragraphs = parent_element.find_all('p')\n",
    "\n",
    "        article_text = ''\n",
    "        if title:\n",
    "            article_text += title + '\\n\\n'\n",
    "\n",
    "        for p in paragraphs:\n",
    "            article_text += p.text.strip() + '\\n'\n",
    "        return article_text\n",
    "    else:\n",
    "        print(\"Error: Parent element containing main content not found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZJePs7gn9Xl",
    "outputId": "bff4d92c-b589-4dc1-b92a-ccf1c503a416"
   },
   "outputs": [],
   "source": [
    "transcripts = []\n",
    "for u in data.URL:\n",
    "    transcript = url_to_transcript(u)\n",
    "    if transcript:\n",
    "        transcripts.append(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuBzcrIYn9Xm"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "!mkdir transcripts\n",
    "\n",
    "# Assuming transcripts is a list of transcripts\n",
    "for url_id, transcript in zip(data.URL_ID, transcripts):\n",
    "    if transcript:\n",
    "        # Serialize the transcript using pickle.dump\n",
    "        with open(f\"transcripts/{url_id}.txt\", \"wb\") as file:\n",
    "            pickle.dump(transcript, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws4RBNGcspNq"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlY0cDmEsGWx",
    "outputId": "a45d9692-c070-447c-b461-d4e23253bc9c"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize an empty dictionary to store loaded transcripts\n",
    "loaded_transcripts = {}\n",
    "\n",
    "\n",
    "# Iterate over URL IDs and load corresponding pickled files\n",
    "for url_id in data.URL_ID:\n",
    "    file_path = f\"transcripts/{url_id}.txt\"\n",
    "    try:\n",
    "        # Load pickled data\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            transcript = pickle.load(file)\n",
    "            # Store the loaded transcript in the dictionary\n",
    "            loaded_transcripts[url_id] = transcript\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "\n",
    "# Now loaded_transcripts dictionary will contain URL IDs as keys and loaded transcripts as values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "FfgS5dKrsR5U",
    "outputId": "2fdcd346-a21b-460d-891d-d8022fffb54e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_df = pd.DataFrame(list(loaded_transcripts.items()), columns=['URL_ID', 'transcript'])\n",
    "# Sort the DataFrame by URL_ID\n",
    "data_df = data_df.sort_values(by='URL_ID').reset_index(drop=True)\n",
    "data_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNBWyRwWsSb2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "5PSviYIdsZTo",
    "outputId": "9d50f323-14d7-47f0-c2fd-02ea2808af09"
   },
   "outputs": [],
   "source": [
    "# # Let's take a look at the updated text\n",
    "# data_clean = pd.DataFrame(data_df.transcript.apply(round1))\n",
    "# data_clean.head()\n",
    "\n",
    "data_df['transcript']= data_df.transcript.apply(round1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HlR0haQtsa9T"
   },
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "q0VU3hOcsili",
    "outputId": "9199bb37-5872-46c7-cebc-face9971fb59"
   },
   "outputs": [],
   "source": [
    "# data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
    "# data_clean.head()\n",
    "\n",
    "data_df['transcript']= data_df.transcript.apply(round2)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-sZlEzzsk2w"
   },
   "outputs": [],
   "source": [
    "data_df.to_pickle(\"corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqX7la6MtVl1"
   },
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLMULSOftYC6",
    "outputId": "c38d161b-42f7-4107-ed39-90611aed3f50"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "stopwords_directory = \"StopWords\"\n",
    "\n",
    "stopwords = []\n",
    "\n",
    "for filename in os.listdir(stopwords_directory):\n",
    "    filepath = os.path.join(stopwords_directory, filename)\n",
    "    with open(filepath, \"r\",encoding=\"ISO-8859-1\") as file:\n",
    "        stopwords.extend(file.read().splitlines())\n",
    "\n",
    "stopwords = list(set(stopwords))\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Oj3ol3XteyR"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(transcript_text):\n",
    "    words = transcript_text.split()\n",
    "\n",
    "    # Filter out stopwords\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word.lower() not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "\n",
    "    # Join the filtered words back into a single string\n",
    "    filtered_transcript_text = ' '.join(filtered_words)\n",
    "\n",
    "    return filtered_transcript_text\n",
    "\n",
    "# Apply the remove_stopwords function to each transcript in the DataFrame\n",
    "data_df['transcript'] = data_df['transcript'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "tV3XwP4itiuP",
    "outputId": "48c4025e-024a-475d-b641-385384b8ef55"
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRvGFG63tfwq"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1gaC_vfsox1",
    "outputId": "5b9ec3a2-167e-4002-f107-2846a03b36e3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('cmudict')\n",
    "\n",
    "# Load positive and negative word lists\n",
    "positive_words = set(open('MasterDictionary/positive-words.txt', encoding='ISO-8859-1').read().splitlines())\n",
    "negative_words = set(open('MasterDictionary/negative-words.txt', encoding='ISO-8859-1').read().splitlines())\n",
    "\n",
    "\n",
    "# Create an empty DataFrame to store the output data\n",
    "output_data = pd.DataFrame(columns=[\"URL_ID\",\"POSITIVE SCORE\", \"NEGATIVE SCORE\", \"POLARITY SCORE\",\n",
    "                                     \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\",\n",
    "                                     \"FOG INDEX\", \"AVG NUMBER OF WORDS PER SENTENCE\", \"COMPLEX WORD COUNT\",\n",
    "                                     \"WORD COUNT\", \"SYLLABLE PER WORD\", \"PERSONAL PRONOUNS\", \"AVG WORD LENGTH\"])\n",
    "\n",
    "# Function to calculate syllables in a word\n",
    "def syllable_count(word):\n",
    "    d = cmudict.dict()\n",
    "    try:\n",
    "        return [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]][0]\n",
    "    except KeyError:\n",
    "        # If word not found in CMU dictionary, approximate syllable count based on length\n",
    "        return max(1, len(word) / 3)\n",
    "\n",
    "# Function to perform textual analysis and compute variables\n",
    "def compute_text_variables(article_text):\n",
    "    # Tokenize text into words and sentences\n",
    "    words = nltk.word_tokenize(article_text)\n",
    "    sentences = nltk.sent_tokenize(article_text)\n",
    "\n",
    "    # Compute positive and negative scores\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "\n",
    "    # Compute polarity score\n",
    "    denominator = (positive_score + negative_score) + 0.000001\n",
    "    polarity_score = (positive_score - negative_score) / denominator\n",
    "\n",
    "    # Compute subjectivity score\n",
    "    total_words = len(words)\n",
    "    subjectivity_score = (positive_score + negative_score) / (total_words + 0.000001)\n",
    "\n",
    "    # Compute other variables\n",
    "    avg_sentence_length = sum(len(nltk.word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "    complex_word_count = sum(1 for word in words if syllable_count(word) > 2)\n",
    "    percentage_complex_words = (complex_word_count / total_words) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = total_words / len(sentences)\n",
    "    avg_syllables_per_word = sum(syllable_count(word) for word in words) / total_words\n",
    "    personal_pronouns = sum(1 for word, pos in nltk.pos_tag(words) if pos in ['PRP', 'PRP$', 'WP', 'WP$'])\n",
    "    avg_word_length = sum(len(word) for word in words) / total_words\n",
    "\n",
    "    # Return computed variables\n",
    "    return (positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length,\n",
    "            percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, total_words,\n",
    "            avg_syllables_per_word, personal_pronouns, avg_word_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "# Iterate over rows of the DataFrame using .iterrows()\n",
    "for index, row in data_df.iterrows():\n",
    "    # Compute text variables for each transcript\n",
    "    text_variables = compute_text_variables(row['transcript'])\n",
    "    \n",
    "    # Extract URL_ID from the row\n",
    "    url_id = row['URL_ID']\n",
    "    \n",
    "    # Combine URL_ID with text variables\n",
    "    text_variables = [url_id] + list(text_variables)\n",
    "    \n",
    "    # Add text variables to the output DataFrame\n",
    "    output_data.loc[index] = text_variables\n",
    "    \n",
    "    # Increment the index for the next row\n",
    "    j += 1\n",
    "    \n",
    "    # Print the text variables for debugging or monitoring\n",
    "    print(text_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_data = pd.merge(data, output_data, on=\"URL_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_output_data.to_excel(\"Output_Data_Structure.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
